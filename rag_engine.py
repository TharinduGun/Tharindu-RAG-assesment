'''Document ingestion, vector search, and LLM calls is handled by RAG system in the rag_engine module'''
from __future__ import annotations
import os
from pathlib import Path
from typing import List, Optional
import logging

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import google.generativeai as genai 

from models import AnswerResponse

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GeminiClient:
    """
    Wrapper around the Google Gemini API used for answer generation.
    """
    def __init__(self, api_key: str, model_name: str | None = None) -> None:
        if not api_key:
            raise ValueError(
            )
        # gemini API key configuration
        if model_name is None:
            model_name = os.getenv("GEMINI_MODEL", "models/gemini-2.5-flash")
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model_name=model_name)

    def generate(self, prompt: str) -> str:
        """
        Generation for the given prompt.

        Parameters
        ----------
        prompt : str
            Full prompt containing both context and question.

        Returns
        -------
        str
            The text generated by the llm. If the API returns no
            text, an empty string is returned.
        """
        response = self._model.generate_content(prompt)
        # The API returns a response object with a `text` attribute.
        return response.text or ""


class RagEngine:
    """
    Retrieval-Augmented Generation system for the document.
    Parameters
    ----------
    pdf_path : Path
        Filesystem path to the PDF document used as the knowledge source.
    chunk_size : int
        Maximum size (in characters) for each text chunk created from the PDF.
    chunk_overlap : int
        Number of overlapping characters between consecutive chunks.
    top_k : int
        Number of relevant chunks to retrieve for each question.
    """

    def __init__(
        self,
        pdf_path: Path,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        top_k: int = 3,
    ) -> None:
        self._pdf_path: Path = pdf_path
        self._chunk_size: int = chunk_size
        self._chunk_overlap: int = chunk_overlap
        self._top_k: int = top_k

        # These will be initialized during ingestion
        self._documents: List[Document] = []
        self._vector_store: Optional[FAISS] = None
        self._llm: Optional[GeminiClient] = None
        self._ingest_document() #ingest the document
        self._init_llm() #initialize the llm

    def _ingest_document(self) -> None:
        """
        Load the PDF, split it into chunks, create embeddings, and build
        the vector store.

        """
        if not self._pdf_path.exists():
            raise FileNotFoundError(f"PDF file not found: {self._pdf_path}")

        # Load the PDF into LangChain Document objects.
        loader = PyPDFLoader(str(self._pdf_path))
        raw_documents: List[Document] = loader.load()

        # Split documents into smaller, overlapping chunks.
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self._chunk_size,
            chunk_overlap=self._chunk_overlap,
        )
        self._documents = text_splitter.split_documents(raw_documents)

        if not self._documents:
            raise RuntimeError("No document chunks were produced from the PDF.")

        # 3. Initialize the embedding model.
        #    'all-MiniLM-L6-v2' is a strong, lightweight, open-source model.
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # 4. Build the FAISS vector store from the chunked documents.
        self._vector_store = FAISS.from_documents(self._documents, embeddings)

        if self._vector_store is None:
            raise RuntimeError("Failed to create FAISS vector store from documents.")
        

    def _init_llm(self) -> None:
        """
        Initialize the language model client used for answer generation.
        """
        api_key = os.getenv("GEMINI_API_KEY")

        logger.info("Initialising Gemini LLM client.")
        self._llm = GeminiClient(api_key=api_key)


    def answer_question(self, question: str) -> AnswerResponse:
        """
        Answer a question using retrieval-augmented generation.

        1. Uses the vector store to retrieve the most relevant chunks.
        2. Builds a prompt that includes the retrieved context and the question.
        3. Invokes the language model to generate an answer.
        4. Returns a structured response containing the question, answer, and
           the concatenated context text.

  
        """
        if self._vector_store is None:
            raise RuntimeError("Vector store is not initialised.")
        if self._llm is None:
            raise RuntimeError("LLM client is not initialised.")

        logger.info("Received question: %s", question)

        # Retrieve the top-k most similar chunks from the vector store.
        retrieved_docs: List[Document] = self._vector_store.similarity_search(
            question, k=self._top_k
        )

        if not retrieved_docs:
            logger.warning("No context found for question; returning empty answer.")
            return AnswerResponse(question=question, answer="", context="")

        # Concatenate the retrieved chunks into a single context string
        context_parts: List[str] = [doc.page_content for doc in retrieved_docs]
        context_text: str = "\n\n---\n\n".join(context_parts)

        # prompt structure for the LLM
        prompt: str = (
            "You are an AI assistant answering questions about the research paper "
            "\"Attention Is All You Need\".\n\n"
            "Use ONLY the information in the context below. If the answer is not "
            "present in the context, say that you do not know.\n\n"
            f"Context:\n{context_text}\n\n"
            f"Question: {question}\n\n"
            "Answer clearly and concisely:\n"
        )
        # 4. Call LLm to generate the answer.
        answer_text: str = self._llm.generate(prompt)

        logger.info("Generated answer of length %d characters.", len(answer_text))

        return AnswerResponse(
            question=question,
            answer=answer_text,
            context=context_text,
        )

